%env MKL_NUM_THREADS=14
%env OMP_NUM_THREADS=14
%env CUDA_VISIBLE_DEVICES=0,1

import os, sys
import time

import multiprocessing as mp
import itertools, functools
import re

import librosa, librosa.display

import scipy
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
import torch, torch.nn as nn, torch.nn.functional as F, torch.autograd as ag


import PyQt5
import matplotlib.pyplot as plt

import IPython
from IPython.display import clear_output
#from tqdm import tqdm_notebook as tqdm_nb, trange

from collections import defaultdict


def next(iterator, default=None, count=1):
    ans = None
    for i in range(count):
        try:
            ans = __builtin__.next(iterator)
        except StopIteration as exc:
            if default is not None:
                return default
            else:
                raise exc
    return ans

def get_spectgorgamm(fname, hop_length=512, n_mels=128):
    y, sr = librosa.load(fname)
    S = librosa.feature.melspectrogram(y, sr=sr, hop_length=hop_length, n_mels=n_mels)
    log_S = librosa.amplitude_to_db(S, ref=np.max)
    return log_S, sr

def plot_spectrogramm(log_S, sr, cmap='viridis'):
    plt.figure(figsize=(20,4))
    librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel', cmap=cmap)
    plt.title('mel power spectrogram')
    plt.colorbar(format='%+02.0f dB')
    plt.tight_layout()

def self_equal(sth):
    return sth == sth

def yield_all(sth, length):
    for i in range(length):
        yield sth[i]

def refine_spectrogramm(x):
    return np.concatenate(( x, -80 * np.ones((128, max(0, 300 - x.shape[1]))) ), axis=-1)[:, :300]

fnames = np.array(next(os.walk('./specs/'))[-1])
ind_tr = np.arange(len(fnames))
Y = np.array([int(fname[: fname.find('-')]) for fname in fnames])
np.random.shuffle(ind_tr)

Y_to_X_tr = defaultdict(list)
for i in range(len(ind_tr)):
    Y_to_X_tr[Y[ind_tr[i]]].append(i)
for key in Y_to_X_tr:
    Y_to_X_tr[key] = np.array(Y_to_X_tr[key])

class GlobalMaxPool1d(nn.Module):
    def forward(self, inp):
        return torch.max(inp, dim=-1)[0]

class VoiceToVec(nn.Module):
    def __init__(self, out_size):
        super(self.__class__, self).__init__()
        self.conv1 = nn.Conv1d(128, 256, 3)
        self.conv2 = nn.Conv1d(256, 512, 3)

        self.gmp1 = GlobalMaxPool1d()
        self.dropout = nn.Dropout(0.2)
        self.dense1 = nn.Linear(512, out_size)

    def forward(self, x):
        x -= torch.min(torch.min(x, 2, keepdim=True)[0], 1, keepdim=True)[0]
        x /= torch.max(torch.max(x, 2, keepdim=True)[0], 1, keepdim=True)[0]

        conv1_out = nn.LeakyReLU()(self.conv1(x))

        conv2_out = nn.LeakyReLU()(self.conv2(conv1_out))

        gmp1_out = self.gmp1(conv2_out)
        dense1_out = self.dense1(self.dropout(gmp1_out))

        return dense1_out

def random_triplet():
    anc_pos_ind, neg_ind = np.random.choice(list(Y_to_X_tr.keys()), size=2, replace=False)

    anchor, positive = np.random.choice(Y_to_X_tr[anc_pos_ind], size=2, replace=False)
    negative = np.random.choice(Y_to_X_tr[neg_ind])
    
    return np.array([anchor, positive, negative])

def random_multiplet(k):
    anc_pos_ind, *neg_inds = np.random.choice(list(Y_to_X_tr.keys()), size=1+k, replace=False)

    anchor, positive = np.random.choice(Y_to_X_tr[anc_pos_ind], size=2, replace=False)
    *negatives, = (np.random.choice(Y_to_X_tr[neg_ind]) for neg_ind in neg_inds)

    return [anchor, positive, negatives]

def iterate_minibatch(batchsize):
    for i in range(batchsize):
        yield random_triplet()

def new_batch(batchsize):
    x_fnames = fnames[ind_tr[np.array(list(iterate_minibatch(batchsize))).T]]
    try:
        ans = np.array([[np.load('./specs/' + fname)[0] for fname in x_fnames[i]] for i in range(3)])
    except BaseException as err:
        globals()['error name'] = x_fnames
        raise err
    return ans

def sim(a, b):
    return -((a-b) ** 2).sum(dim=-1)

model = VoiceToVec(100)
model.load_state_dict(torch.load('./saved-models/voice2vec-750.pt'))
model = model.cpu()  # model.cuda(0)

opt = torch.optim.Adam(model.parameters())

losses = [1]
losses_moving_avg = [1]

torch.set_default_tensor_type('torch.FloatTensor')

batch_size = 20
i = -1

while 1:
    i += 1
    model.train(True)
    
    anchor, positive, negative = map(torch.autograd.Variable, map(torch.Tensor, new_batch(batch_size)))
    
    anchor_vec, positive_vec, negative_vec = map(model, (anchor, positive, negative))
    
    loss = F.relu(1 + sim(anchor_vec, negative_vec) - sim(anchor_vec, positive_vec)).mean()
    
    loss.backward()
    if i % 20 == 0:
        opt.step()
        opt.zero_grad()
    
    if (len(losses) * batch_size) % 5000 == 0:
        torch.save(model.state_dict(), './saved-models/voice2vec-{}.pt'.format(str(time.time())))
    
    losses.append(loss.cpu().data.numpy()[0])
    losses_moving_avg.append(losses[-1]*0.03 + losses_moving_avg[-1]*0.97)
    if ind_epoch % 1 == 0:
        IPython.display.clear_output(True)
        plt.plot(losses_moving_avg)
        plt.scatter(np.arange(len(losses)), losses, alpha=0.1)
        plt.show()


